{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26248cc6-5877-4e22-9463-ed9a6ff083d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c1d0ff-d362-47a1-bb8f-195b00bd99a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.13.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4.0.0dev,>=3.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (3.20.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.20.4)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.1.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.34.3)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.35.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.56.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.46.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.46.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (59.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.21)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042734cd-1497-4900-82c3-b7a4dc0a673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (2.1.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.35.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.56.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (59.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.12)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media>=2.3.2->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media>=2.3.2->google-cloud-storage) (2.21)\n",
      "Installing collected packages: google-cloud-storage\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.12 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.48.0 which is incompatible.\n",
      "kfp 1.8.12 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-storage-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install {USER_FLAG} -U google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036f5ce8-a912-472c-8194-2e079468f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery[all] in /opt/conda/lib/python3.7/site-packages (2.34.3)\n",
      "Collecting google-cloud-bigquery[all]\n",
      "  Downloading google_cloud_bigquery-3.2.0-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (1.46.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (2.27.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (2.8.2)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (3.20.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (2.3.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (2.3.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (2.8.0)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (21.3)\n",
      "Requirement already satisfied: pyarrow<9.0dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (8.0.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (1.20.4)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (2.13.1)\n",
      "Collecting opentelemetry-api>=1.1.0\n",
      "  Downloading opentelemetry_api-1.11.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (1.3.5)\n",
      "Collecting geopandas<1.0dev,>=0.9.0\n",
      "  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Shapely<2.0dev,>=1.6.0\n",
      "  Downloading Shapely-1.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-sdk>=1.1.0\n",
      "  Downloading opentelemetry_sdk-1.11.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting db-dtypes<2.0.0dev,>=0.3.0\n",
      "  Downloading db_dtypes-1.0.2-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: ipython!=8.1.0,>=7.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (7.33.0)\n",
      "Requirement already satisfied: tqdm<5.0.0dev,>=4.7.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery[all]) (4.64.0)\n",
      "Collecting opentelemetry-instrumentation>=0.20b0\n",
      "  Downloading opentelemetry_instrumentation-0.31b0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy<2.0dev,>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from db-dtypes<2.0.0dev,>=0.3.0->google-cloud-bigquery[all]) (1.21.6)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fiona>=1.8\n",
      "  Downloading Fiona-1.8.21-cp37-cp37m-manylinux2014_x86_64.whl (16.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (1.56.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (1.35.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (1.46.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery[all]) (1.1.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.7/site-packages (from grpcio<2.0dev,>=1.38.1->google-cloud-bigquery[all]) (1.16.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (59.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (3.0.29)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (5.2.1.post0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (2.12.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.7.5)\n",
      "Requirement already satisfied: Deprecated>=1.2.6 in /opt/conda/lib/python3.7/site-packages (from opentelemetry-api>=1.1.0->google-cloud-bigquery[all]) (1.2.13)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from opentelemetry-instrumentation>=0.20b0->google-cloud-bigquery[all]) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from opentelemetry-sdk>=1.1.0->google-cloud-bigquery[all]) (3.10.0.2)\n",
      "Collecting opentelemetry-semantic-conventions==0.30b1\n",
      "  Downloading opentelemetry_semantic_conventions-0.30b1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-bigquery[all]) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.0->google-cloud-bigquery[all]) (2022.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery[all]) (1.26.9)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas<1.0dev,>=0.9.0->google-cloud-bigquery[all]) (8.1.3)\n",
      "Collecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas<1.0dev,>=0.9.0->google-cloud-bigquery[all]) (21.4.0)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (4.8)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery[all]) (1.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython!=8.1.0,>=7.0.1->google-cloud-bigquery[all]) (0.2.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery[all]) (2.21)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=4.0->fiona>=1.8->geopandas<1.0dev,>=0.9.0->google-cloud-bigquery[all]) (4.11.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery[all]) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click>=4.0->fiona>=1.8->geopandas<1.0dev,>=0.9.0->google-cloud-bigquery[all]) (3.8.0)\n",
      "Installing collected packages: Shapely, pyproj, opentelemetry-semantic-conventions, munch, opentelemetry-api, opentelemetry-sdk, opentelemetry-instrumentation, db-dtypes, cligj, click-plugins, fiona, geopandas, google-cloud-bigquery\n",
      "\u001b[33m  WARNING: The script pyproj is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts opentelemetry-bootstrap and opentelemetry-instrument are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script fio is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-aiplatform 1.14.0 requires google-cloud-bigquery<3.0.0dev,>=1.15.0, but you have google-cloud-bigquery 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Shapely-1.8.2 click-plugins-1.1.1 cligj-0.7.2 db-dtypes-1.0.2 fiona-1.8.21 geopandas-0.10.2 google-cloud-bigquery-3.2.0 munch-2.5.0 opentelemetry-api-1.11.1 opentelemetry-instrumentation-0.31b0 opentelemetry-sdk-1.11.1 opentelemetry-semantic-conventions-0.30b1 pyproj-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install {USER_FLAG} -U \"google-cloud-bigquery[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca2dd61-b109-4739-82c8-55d5c9c107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb2b05c-c2bc-47d1-92d3-32b739d4e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  mle-creditcard-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Get your Google Cloud project ID from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dedc2df-3f87-44d2-ad8e-ff0ec890e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1245e8be-23d9-477a-8ef8-e1e4e5bfc1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://creditcard_default_project\"\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd8c8da-5cf9-4a96-be80-8cd31e02e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be603140-2482-43c3-9370-0e84ea336a72",
   "metadata": {},
   "source": [
    "# Import Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad96ce5c-c8ba-42dd-bda3-d64a327c87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377efd3a-c543-4ec9-a142-b23b045d8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify accelerator\n",
    "TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4cb95d1-6e8a-43bb-9a10-48e7d1faa5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest None None\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest None None\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-8\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-8\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5d08e39-155f-443c-b4d2-0803a3c76019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f0854f-dae0-4280-81fa-e94d50519327",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_SOURCE = \"bq://mle-creditcard-project.project_datasets.creditcard_default_taiwan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ae9117-6788-4091-b48e-f7ea4c3e0cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ID         30000 non-null  Int64  \n",
      " 1   LIMIT_BAL  30000 non-null  Int64  \n",
      " 2   SEX        30000 non-null  object \n",
      " 3   EDUCATION  30000 non-null  object \n",
      " 4   MARRIAGE   30000 non-null  object \n",
      " 5   AGE        30000 non-null  Int64  \n",
      " 6   PAY_0      30000 non-null  Int64  \n",
      " 7   PAY_2      30000 non-null  Int64  \n",
      " 8   PAY_3      30000 non-null  Int64  \n",
      " 9   PAY_4      30000 non-null  Int64  \n",
      " 10  PAY_5      30000 non-null  Int64  \n",
      " 11  PAY_6      30000 non-null  Int64  \n",
      " 12  BILL_AMT1  30000 non-null  Int64  \n",
      " 13  BILL_AMT2  30000 non-null  Int64  \n",
      " 14  BILL_AMT3  30000 non-null  Int64  \n",
      " 15  BILL_AMT4  30000 non-null  Int64  \n",
      " 16  BILL_AMT5  30000 non-null  Int64  \n",
      " 17  BILL_AMT6  30000 non-null  Int64  \n",
      " 18  PAY_AMT1   30000 non-null  Int64  \n",
      " 19  PAY_AMT2   30000 non-null  Int64  \n",
      " 20  PAY_AMT3   30000 non-null  Int64  \n",
      " 21  PAY_AMT4   30000 non-null  Int64  \n",
      " 22  PAY_AMT5   30000 non-null  Int64  \n",
      " 23  PAY_AMT6   30000 non-null  Int64  \n",
      " 24  default_   30000 non-null  boolean\n",
      "dtypes: Int64(21), boolean(1), object(3)\n",
      "memory usage: 6.4+ MB\n",
      "None\n",
      "The mean and stds for each column are: {'ID': {'mean': 15000.5, 'std': 8660.398374208891}, 'LIMIT_BAL': {'mean': 167484.32266666667, 'std': 129747.66156720239}, 'AGE': {'mean': 35.4855, 'std': 9.217904068090188}, 'PAY_0': {'mean': -0.0167, 'std': 1.1238015279973348}, 'PAY_2': {'mean': -0.13376666666666667, 'std': 1.1971859730345533}, 'PAY_3': {'mean': -0.1662, 'std': 1.1968675684465737}, 'PAY_4': {'mean': -0.22066666666666668, 'std': 1.1691386224023375}, 'PAY_5': {'mean': -0.2662, 'std': 1.1331874060027483}, 'PAY_6': {'mean': -0.2911, 'std': 1.1499876256079027}, 'BILL_AMT1': {'mean': 51223.3309, 'std': 73635.86057552956}, 'BILL_AMT2': {'mean': 49179.07516666667, 'std': 71173.76878252835}, 'BILL_AMT3': {'mean': 47013.1548, 'std': 69349.38742703683}, 'BILL_AMT4': {'mean': 43262.94896666666, 'std': 64332.85613391631}, 'BILL_AMT5': {'mean': 40311.40096666667, 'std': 60797.15577026487}, 'BILL_AMT6': {'mean': 38871.7604, 'std': 59554.10753674573}, 'PAY_AMT1': {'mean': 5663.5805, 'std': 16563.280354025766}, 'PAY_AMT2': {'mean': 5921.1635, 'std': 23040.870402057237}, 'PAY_AMT3': {'mean': 5225.6815, 'std': 17606.961469803104}, 'PAY_AMT4': {'mean': 4826.076866666666, 'std': 15666.159744032007}, 'PAY_AMT5': {'mean': 4799.387633333334, 'std': 15278.30567914479}, 'PAY_AMT6': {'mean': 5215.502566666667, 'std': 17777.465775435296}}\n",
      "Copying file://mean_and_std.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and std across all rows\n",
    "\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe()\n",
    "\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def calculate_mean_and_std(df):\n",
    "    # Calculate mean and std for each applicable column\n",
    "    mean_and_std = {}\n",
    "    dtypes = list(zip(df.dtypes.index, map(str, df.dtypes)))\n",
    "    print(df.info())\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == \"float32\" or dtype == \"float64\" or dtype == \"Int64\":\n",
    "            mean_and_std[column] = {\n",
    "                \"mean\": df[column].mean(),\n",
    "                \"std\": df[column].std(),\n",
    "            }\n",
    "\n",
    "    return mean_and_std\n",
    "\n",
    "\n",
    "dataframe = download_table(BQ_SOURCE)\n",
    "dataframe = clean_dataframe(dataframe)\n",
    "mean_and_std = calculate_mean_and_std(dataframe)\n",
    "\n",
    "print(\"The mean and stds for each column are: \" + str(mean_and_std))\n",
    "\n",
    "# Write to a file\n",
    "MEAN_AND_STD_JSON_FILE = \"mean_and_std.json\"\n",
    "\n",
    "with open(MEAN_AND_STD_JSON_FILE, \"w\") as outfile:\n",
    "    json.dump(mean_and_std, outfile)\n",
    "\n",
    "# Save to the staging bucket\n",
    "! gsutil cp {MEAN_AND_STD_JSON_FILE} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e81156aa-eab1-4091-9d41-2fb96e78d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/153707083586/locations/us-central1/datasets/5452377007071428608/operations/2821959883325177856\n",
      "TabularDataset created. Resource name: projects/153707083586/locations/us-central1/datasets/5452377007071428608\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/153707083586/locations/us-central1/datasets/5452377007071428608')\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"ccd-ds\", bq_source=BQ_SOURCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c78988a9-323a-4349-b005-92a12d4703ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"ccd_train_job\" + TIMESTAMP\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "# EPOCHS = 20\n",
    "# BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    # \"--epochs=\" + str(EPOCHS),\n",
    "    # \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    \"--mean_and_std_json_file=\" + f\"{BUCKET_URI}/{MEAN_AND_STD_JSON_FILE}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c0c607c-8620-4882-a775-94616dcf90ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--epochs', dest='epochs',\n",
    "#                     default=10, type=int,\n",
    "#                     help='Number of epochs.')\n",
    "# parser.add_argument('--batch_size', dest='batch_size',\n",
    "#                     default=10, type=int,\n",
    "#                     help='Batch size.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='Distributed training strategy.')\n",
    "parser.add_argument('--mean_and_std_json_file', dest='mean_and_std_json_file', type=str,\n",
    "                    help='GCS URI to the JSON file with pre-calculated column means and standard deviations.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "def extract_bucket_and_prefix_from_gcs_path(gcs_path: str):\n",
    "    \"\"\"Given a complete GCS path, return the bucket name and prefix as a tuple.\n",
    "\n",
    "    Example Usage:\n",
    "\n",
    "        bucket, prefix = extract_bucket_and_prefix_from_gcs_path(\n",
    "            \"gs://example-bucket/path/to/folder\"\n",
    "        )\n",
    "\n",
    "        # bucket = \"example-bucket\"\n",
    "        # prefix = \"path/to/folder\"\n",
    "\n",
    "    Args:\n",
    "        gcs_path (str):\n",
    "            Required. A full path to a Google Cloud Storage folder or resource.\n",
    "            Can optionally include \"gs://\" prefix or end in a trailing slash \"/\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, Optional[str]]\n",
    "            A (bucket, prefix) pair from provided GCS path. If a prefix is not\n",
    "            present, a None will be returned in its place.\n",
    "    \"\"\"\n",
    "    if gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = gcs_path[5:]\n",
    "    if gcs_path.endswith(\"/\"):\n",
    "        gcs_path = gcs_path[:-1]\n",
    "\n",
    "    gcs_parts = gcs_path.split(\"/\", 1)\n",
    "    gcs_bucket = gcs_parts[0]\n",
    "    gcs_blob_prefix = None if len(gcs_parts) == 1 else gcs_parts[1]\n",
    "\n",
    "    return (gcs_bucket, gcs_blob_prefix)\n",
    "\n",
    "# Download means and std\n",
    "def download_mean_and_std(mean_and_std_json_file):\n",
    "    \"\"\"Download mean and std for each column\"\"\"\n",
    "    import json\n",
    "\n",
    "    bucket, file_path = extract_bucket_and_prefix_from_gcs_path(mean_and_std_json_file)\n",
    "    download_blob(bucket_name=bucket, source_blob_name=file_path, destination_file_name=file_path)\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.loads(file.read())\n",
    "\n",
    "mean_and_std = download_mean_and_std(args.mean_and_std_json_file)\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"default_\"\n",
    "UNUSED_COLUMNS = ['ID', 'SEX', 'EDUCATION', 'MARRIAGE']\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Possible categorical values\n",
    "MARRIAGE = ['Married', 'Single', 'Other', '0']\n",
    "EDUCATION = ['University', 'Graduate school', 'High School',\n",
    "             'Unknown', 'Others', '0']\n",
    "SEX = ['F', 'M']\n",
    "\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "df_train = clean_dataframe(df_train)\n",
    "#     df_validation = clean_dataframe(df_validation)\n",
    "df_validation = clean_dataframe(df_validation)\n",
    "\n",
    "_CATEGORICAL_TYPES = {\n",
    "    \"marriage\": pd.api.types.CategoricalDtype(categories=MARRIAGE),\n",
    "    \"education\": pd.api.types.CategoricalDtype(categories=EDUCATION),\n",
    "    \"sex\": pd.api.types.CategoricalDtype(categories=SEX),\n",
    "}\n",
    "\n",
    "\n",
    "def standardize(df, mean_and_std):\n",
    "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
    "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
    "    deviation becomes 1. This can help the model converge during training.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df\n",
    "\n",
    "    Returns:\n",
    "      Input df with the numerical columns scaled to z-scores\n",
    "    \"\"\"\n",
    "    dtypes = list(zip(df.dtypes.index, map(str, df.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == \"float32\":\n",
    "            df[column] -= mean_and_std[column][\"mean\"]\n",
    "            df[column] /= mean_and_std[column][\"std\"]\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    cat_columns = df.select_dtypes([\"object\"]).columns\n",
    "\n",
    "    df[cat_columns] = df[cat_columns].apply(\n",
    "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name])\n",
    "    )\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train,\n",
    "    df_validation,\n",
    "    mean_and_std\n",
    "):\n",
    "    df_train = preprocess(df_train)\n",
    "    df_validation = preprocess(df_validation)\n",
    "\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    # Join train_x and eval_x to normalize on overall means and standard\n",
    "    # deviations. Then separate them again.\n",
    "    all_x = pd.concat([df_train_x, df_validation_x], keys=[\"train\", \"eval\"])\n",
    "    # all_x = standardize(all_x, mean_and_std)\n",
    "    df_train_x, df_validation_x = all_x.xs(\"train\"), all_x.xs(\"eval\")\n",
    "\n",
    "    # y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "    # y_validation = np.asarray(df_validation_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x)\n",
    "    x_test = np.asarray(df_validation_x)\n",
    "\n",
    "    # label encode the target column\n",
    "    le = LabelEncoder()\n",
    "    labels = df_train_y\n",
    "    y = le.fit_transform(labels) \n",
    "    y_train = le.fit_transform(labels) \n",
    "    \n",
    "    labels = df_validation_y\n",
    "    y_validation = le.fit_transform(labels) \n",
    "\n",
    "    # dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    # dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return x_train, y_train, x_test, y_validation\n",
    "\n",
    "# Create datasets\n",
    "x_train, y_train, x_test, y_validation = convert_dataframe_to_dataset(df_train, df_validation, mean_and_std)\n",
    "\n",
    "# # Shuffle train set\n",
    "# dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "# def create_model(num_features):\n",
    "    # Create model\n",
    "    # xg_model = XGBClassifier()\n",
    "    # return model\n",
    "\n",
    "# # Create the model\n",
    "# with strategy.scope():\n",
    "#     model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "# NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "# GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
    "# dataset_train = dataset_train.batch(GLOBAL_BATCH_SIZE)\n",
    "# dataset_validation = dataset_validation.batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "xg_model = XGBClassifier()\n",
    "xg_model.fit(x_train,y_train)\n",
    "\n",
    "artifact_filename = 'model.pkl'\n",
    "\n",
    "# Save model artifact to local filesystem (doesn't persist)\n",
    "local_path = artifact_filename\n",
    "with open(local_path, 'wb') as model_file:\n",
    "  pickle.dump(xg_model, model_file)\n",
    "\n",
    "# Upload model artifact to Cloud Storage\n",
    "model_directory = os.environ['AIP_MODEL_DIR']\n",
    "storage_path = os.path.join(model_directory, artifact_filename)\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "blob.upload_from_filename(local_path)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3fa432b-51bc-4ebf-9292-2fc7d0da461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://creditcard_default_project/aiplatform-2022-06-22-03:18:33.896-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://creditcard_default_project/aiplatform-custom-training-2022-06-22-03:18:34.045 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3683865605229772800?project=153707083586\n",
      "CustomTrainingJob projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1796998098849890304?project=153707083586\n",
      "CustomTrainingJob projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/153707083586/locations/us-central1/trainingPipelines/3683865605229772800\n",
      "Model available at projects/153707083586/locations/us-central1/models/6159785196196462592\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"ccd-\" + TIMESTAMP\n",
    "\n",
    "# Start the training\n",
    "# if TRAIN_GPU:\n",
    "#     model = job.run(\n",
    "#         dataset=dataset,\n",
    "#         model_display_name=MODEL_DISPLAY_NAME,\n",
    "#         bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "#         args=CMDARGS,\n",
    "#         replica_count=1,\n",
    "#         machine_type=TRAIN_COMPUTE,\n",
    "#         accelerator_type=TRAIN_GPU.name,\n",
    "#         accelerator_count=TRAIN_NGPU,\n",
    "#     )\n",
    "# else:\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bd85c-c924-4f94-857b-a2ebf025d777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
