{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "    create_custom_training_job_op_from_component,\n",
    ")\n",
    "\n",
    "from kfp.dependencies import PROJECT_ID, ROOT_DIR, sql_query\n",
    "from kfp.v2 import compiler, dsl\n",
    "from pipelines.kfp.helpers import generate_query\n",
    "\n",
    "# from pipelines.kfp.helpers import copy_artifact\n",
    "from pipelines.kfp.ingest import bq_extract_data, bq_query_to_table\n",
    "from pipelines.training import train_xgboost_model\n",
    "\n",
    "print(ROOT_DIR)\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"xgboost-train-pipeline\")\n",
    "def xgboost_pipeline(\n",
    "    # project_id: str,\n",
    "    # project_location: str,\n",
    "    # pipeline_files_gcs_path: str,\n",
    "    # ingestion_project_id: str,\n",
    "    # tfdv_schema_filename: str,\n",
    "    # tfdv_train_stats_path: str,\n",
    "    # model_name: str,\n",
    "    # model_label: str,\n",
    "    # dataset_id: str,\n",
    "    # dataset_location: str,\n",
    "    # ingestion_dataset_id: str,\n",
    "    # timestamp: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Query a view from BQ\n",
    "    Extract the view to GCS\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sql_query = generate_query(\n",
    "        input_file=ROOT_DIR\n",
    "        / \"pipelines\"\n",
    "        / \"kfp\"\n",
    "        / \"ingest\"\n",
    "        / \"queries\"\n",
    "        / \"query_bq.sql\"\n",
    "    )\n",
    "\n",
    "    ingest = bq_query_to_table(\n",
    "        query=sql_query,\n",
    "        bq_client_project_id=None,\n",
    "        destination_project_id=PROJECT_ID,\n",
    "        dataset_id=\"dwh_pacific_torus\",\n",
    "        table_id=\"credit_card_default\",\n",
    "        dataset_location=\"US\",\n",
    "        query_job_config=None,\n",
    "    )\n",
    "    # .set_display_name(\"Ingest data\")\n",
    "\n",
    "    # ingest_to_gcs = (\n",
    "    #     bq_extract_data(\n",
    "    #         source_project_id=\"pacific-torus-347809\",\n",
    "    #         source_dataset_id=\"dwh_pacific_torus\",\n",
    "    #         source_table_id=\"credit_card_default\",\n",
    "    #         destination_project_id=\"pacific-torus-347809\",\n",
    "    #         destination_bucket=\"mle-dwh-torus\",\n",
    "    #         destination_file=\"raw/credit_cards.csv\",\n",
    "    #         dataset_location=\"US\",\n",
    "    #     )\n",
    "    #     # .after(ingest)\n",
    "    #     # .set_display_name(\"Export to GCS\")\n",
    "    # )\n",
    "\n",
    "\n",
    "def compile():\n",
    "\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=xgboost_pipeline,\n",
    "        pipeline_name=\"xgboost-train-pipeline\",\n",
    "        package_path=\"training.yaml\",\n",
    "        type_check=True,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    custom_train_job = create_custom_training_job_op_from_component(\n",
    "        component_spec=train_xgboost_model,\n",
    "        replica_count=1,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "\n",
    "    compile()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
