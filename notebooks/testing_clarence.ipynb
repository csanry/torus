{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLARENCE\n",
    "#### Notebook primarily for testing components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18 Jun \n",
    "Pipeline not running as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "    create_custom_training_job_op_from_component,\n",
    ")\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from pipelines.kfp.helpers import generate_query\n",
    "\n",
    "from pipelines.kfp_components.ingest import bq_extract_data, bq_query_to_table\n",
    "from pipelines.kfp_components.training import train_xgboost_model\n",
    "from pipelines.kfp_components.dependencies import PROJECT_ID, ROOT_DIR, sql_query\n",
    "\n",
    "print(ROOT_DIR)\n",
    "\n",
    "@dsl.pipeline(name=\"xgboost-train-pipeline\")\n",
    "def xgboost_pipeline(\n",
    "    # project_id: str,\n",
    "    # project_location: str,\n",
    "    # pipeline_files_gcs_path: str,\n",
    "    # ingestion_project_id: str,\n",
    "    # tfdv_schema_filename: str,\n",
    "    # tfdv_train_stats_path: str,\n",
    "    # model_name: str,\n",
    "    # model_label: str,\n",
    "    # dataset_id: str,\n",
    "    # dataset_location: str,\n",
    "    # ingestion_dataset_id: str,\n",
    "    # timestamp: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Query a view from BQ\n",
    "    Extract the view to GCS\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sql_query = generate_query(\n",
    "        input_file=ROOT_DIR\n",
    "        / \"pipelines\"\n",
    "        / \"kfp\"\n",
    "        / \"ingest\"\n",
    "        / \"queries\"\n",
    "        / \"query_bq.sql\"\n",
    "    )\n",
    "\n",
    "    ingest = bq_query_to_table(\n",
    "        query=sql_query,\n",
    "        bq_client_project_id=None,\n",
    "        destination_project_id=PROJECT_ID,\n",
    "        dataset_id=\"dwh_pacific_torus\",\n",
    "        table_id=\"credit_card_default\",\n",
    "        dataset_location=\"US\",\n",
    "        query_job_config=None,\n",
    "    )\n",
    "    # .set_display_name(\"Ingest data\")\n",
    "\n",
    "    # ingest_to_gcs = (\n",
    "    #     bq_extract_data(\n",
    "    #         source_project_id=\"pacific-torus-347809\",\n",
    "    #         source_dataset_id=\"dwh_pacific_torus\",\n",
    "    #         source_table_id=\"credit_card_default\",\n",
    "    #         destination_project_id=\"pacific-torus-347809\",\n",
    "    #         destination_bucket=\"mle-dwh-torus\",\n",
    "    #         destination_file=\"raw/credit_cards.csv\",\n",
    "    #         dataset_location=\"US\",\n",
    "    #     )\n",
    "    #     # .after(ingest)\n",
    "    #     # .set_display_name(\"Export to GCS\")\n",
    "    # )\n",
    "\n",
    "\n",
    "def compile():\n",
    "\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=xgboost_pipeline,\n",
    "        pipeline_name=\"xgboost-train-pipeline\",\n",
    "        package_path=\"training.yaml\",\n",
    "        type_check=True,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    custom_train_job = create_custom_training_job_op_from_component(\n",
    "        component_spec=train_xgboost_model,\n",
    "        replica_count=1,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "\n",
    "    compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22 Jun Test ingest component step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "from kfp.v2 import compiler, dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_op = kfp.components.load_component_from_file(\"../pipelines/kfp_components/ingest/ingest_component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ingest_op.component_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='testing pipeline',\n",
    "   description='NIL'\n",
    ")\n",
    "def xgboost_pipeline(\n",
    "):\n",
    "    ingest = ingest_op(\n",
    "        source_project_id=\"pacific-torus-347809\",\n",
    "        source_dataset_id=\"dwh_pacific_torus\",\n",
    "        source_table_id=\"credit_card_defaults\",\n",
    "        destination_project_id=\"pacific-torus-347809\",\n",
    "        destination_bucket=\"mle-dwh-torus\",\n",
    "        destination_file=\"raw/test.csv\",\n",
    "        dataset_location=\"US\",\n",
    "        extract_job_config=\"None\",\n",
    "    ) # .apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=xgboost_pipeline,\n",
    "    pipeline_name=\"xgboost-train-pipeline\",\n",
    "    package_path=\"training.json\",\n",
    "    type_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23 Jun Simple pipeline trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import Dataset, Output, component, OutputPath, Artifact, Input\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp.v2 import compiler, dsl\n",
    "\n",
    "aip.init(project=\"pacific-torus-347809\", staging_bucket=\"gs://mle-dwh-torus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"pacific-torus.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_extract_data(\n",
    "    source_project_id: str,\n",
    "    source_table_url: str,\n",
    "    destination_project_id: str,\n",
    "    destination_bucket: str, \n",
    "    destination_file: str,\n",
    "    dataset_location: str,\n",
    "    extract_job_config: dict = None,\n",
    ") -> NamedTuple('outputs', [('dataset_gcs_uri', str), \n",
    "    ('dataset_gcs_directory', str)]\n",
    "):\n",
    "\n",
    "    import logging\n",
    "    import os \n",
    "    from google.cloud.exceptions import GoogleCloudError\n",
    "    from google.cloud import bigquery, storage\n",
    "\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"pacific-torus.json\"\n",
    "\n",
    "    # logging.config.fileConfig(LOGGING_CONF)\n",
    "    logger = logging.getLogger(\"root\")\n",
    "\n",
    "    storage_client = storage.Client(project=destination_project_id)\n",
    "\n",
    "    if not storage.Bucket(storage_client, destination_bucket).exists():\n",
    "        bucket = storage_client.bucket(destination_bucket)\n",
    "        bucket.storage_class = \"STANDARD\" \n",
    "        storage_client.create_bucket(\n",
    "            bucket_or_name=bucket, location=\"ASIA-SOUTHEAST1\", project=destination_project_id\n",
    "        )\n",
    "        logger.info(f\"Bucket created {destination_bucket}\")\n",
    "\n",
    "    full_table_url = f\"{source_project_id}.{source_table_url}\"\n",
    "    table = bigquery.table.Table(table_ref=full_table_url)\n",
    "\n",
    "    if extract_job_config is None:\n",
    "        extract_job_config = {}\n",
    "    if destination_file.endswith(\".json\"):\n",
    "        extract_job_config = {\"destination_format\": \"NEWLINE_DELIMITED_JSON\"}\n",
    "    job_config = bigquery.ExtractJobConfig(**extract_job_config)\n",
    "\n",
    "\n",
    "    dataset_gcs_uri = f\"gs://{destination_bucket}/{destination_file}\"\n",
    " \n",
    "    bq_client = bigquery.Client(project=destination_project_id)\n",
    "\n",
    "    logger.info(f\"Extract {source_table_url} to {dataset_gcs_uri}\")\n",
    "    extract_job = bq_client.extract_table(\n",
    "        source=table,\n",
    "        destination_uris=dataset_gcs_uri,\n",
    "        job_config=job_config,\n",
    "        location=dataset_location,\n",
    "    )\n",
    "\n",
    "    dataset_gcs_directory = os.path.dirname(dataset_gcs_uri)\n",
    "\n",
    "    try:\n",
    "        extract_job.result()\n",
    "        logger.info(f\"Table extracted: {dataset_gcs_uri}\")\n",
    "    except GoogleCloudError as e:\n",
    "        logger.error(e)\n",
    "        logger.error(extract_job.error_result)\n",
    "        logger.error(extract_job.errors)\n",
    "        raise e\n",
    "\n",
    "    return (dataset_gcs_uri, dataset_gcs_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(\n",
    "    input_file: str,\n",
    "    output_bucket: str,\n",
    "    output_file: str\n",
    ") -> Artifact: \n",
    "\n",
    "    import pandas as pd\n",
    "    from functools import reduce\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    df.columns = [col.lower().strip() for col in df.columns] \n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df.rename({'default_payment_next_month': \"default\"}, inplace=True)\n",
    "    df.loc[df['education'] == '0', 'education'] = 'Unknown'\n",
    "    df.loc[df['marriage'] == '0', 'marriage'] = 'Other'\n",
    "    sex = pd.get_dummies(df.sex, prefix='gender')\n",
    "    education = pd.get_dummies(df.education, prefix='ed')\n",
    "    marriage = pd.get_dummies(df.marriage, prefix='mstatus')\n",
    "    frames = [df, sex, education, marriage]\n",
    "    final = reduce(lambda l, r: pd.concat([l, r], axis=1), frames)\n",
    "    final.drop(['default_payment_next_month', 'sex', 'education', 'marriage'], axis=1, inplace=True)\n",
    "\n",
    "    output_path = f\"gs://mle-dwh-torus/{output_bucket}/{output_file}\" \n",
    "    final.to_csv(output_path, index=False)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def train_test_split(\n",
    "    input_file: InputPath(\"CSV\"),\n",
    "    output_bucket: str,\n",
    ") -> NamedTuple(\"outputs\", [\n",
    "    (\"train_data\", Artifact),\n",
    "    (\"test_data\", Artifact)\n",
    "]):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=2022)\n",
    "\n",
    "    output_train_path = f\"gs://mle-dwh-torus/{output_bucket}/train.csv\"\n",
    "    output_test_path = f\"gs://mle-dwh-torus/{output_bucket}/test.csv\" \n",
    "\n",
    "    train.to_csv(output_train_path)\n",
    "    test.to_csv(output_test_path)\n",
    "\n",
    "    return (train, test)\n",
    "\n",
    "\n",
    "def read_data(\n",
    "    a: InputPath(\"CSV\"),\n",
    "): \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "kfp.components.func_to_container_op(\n",
    "    bq_extract_data,\n",
    "    extra_code=\"from typing import NamedTuple\",\n",
    "    output_component_file='ingest_component.yaml', \n",
    "    base_image='gcr.io/pacific-torus-347809/mle-fp/base:latest')\n",
    "\n",
    "kfp.components.func_to_container_op(\n",
    "    basic_preprocessing,\n",
    "    extra_code=\"from kfp.v2.dsl import Dataset, InputPath, OutputPath\",\n",
    "    output_component_file='basic_preprocessing_component.yaml', \n",
    "    base_image='gcr.io/pacific-torus-347809/mle-fp/base:latest',\n",
    "    packages_to_install=[\"fsspec\", \"gcsfs\"])\n",
    "\n",
    "kfp.components.func_to_container_op(\n",
    "    train_test_split,\n",
    "    extra_code=\"from kfp.v2.dsl import Dataset, InputPath\",\n",
    "    output_component_file='tts_component.yaml', \n",
    "    base_image='gcr.io/pacific-torus-347809/mle-fp/base:latest',\n",
    "    packages_to_install=[\"fsspec\", \"gcsfs\", \"sklearn\"])\n",
    "\n",
    "kfp.components.func_to_container_op(\n",
    "    read_data,\n",
    "    extra_code=\"from kfp.v2.dsl import Dataset, InputPath\",\n",
    "    output_component_file='data_component.yaml', \n",
    "    base_image='gcr.io/pacific-torus-347809/mle-fp/base:latest',\n",
    "    packages_to_install=[\"fsspec\", \"gcsfs\", \"sklearn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_op = kfp.components.load_component_from_file(\"./ingest_component.yaml\")\n",
    "remove_na_op = kfp.components.load_component_from_file(\"./basic_preprocessing_component.yaml\")\n",
    "tts_op = kfp.components.load_component_from_file(\"./tts_component.yaml\")\n",
    "\n",
    "data_op = kfp.components.load_component_from_file(\"./data_component.yaml\")\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline/\".format(\"gs://mle-dwh-torus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='testing pipeline',\n",
    "   description='NIL',\n",
    "   pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def xgboost_test_pipeline(\n",
    "):\n",
    "    ingest = ingest_op(\n",
    "        source_project_id=\"pacific-torus-347809\",\n",
    "        source_table_url=\"dwh_pacific_torus.credit_card_defaults\",\n",
    "        destination_project_id=\"pacific-torus-347809\",\n",
    "        destination_bucket=\"mle-dwh-torus\",\n",
    "        destination_file=\"raw/new_test.csv\",\n",
    "        dataset_location=\"US\",\n",
    "        extract_job_config={},\n",
    "    ) # .apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "    remove_na = remove_na_op(\n",
    "        input_file=ingest.outputs[\"dataset_gcs_uri\"],\n",
    "        output_bucket=\"int\",\n",
    "        output_file=\"ccd2.csv\"\n",
    "    )\n",
    "    \n",
    "    tts = tts_op(\n",
    "        input=remove_na.output,\n",
    "        output_bucket=\"fin\"\n",
    "    )\n",
    "\n",
    "    data = data_op(\n",
    "        a=tts.outputs[\"train_data\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../pacific-torus.json\"\n",
    "from datetime import datetime\n",
    "id = datetime.now().strftime(f\"%d%H%M\")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=xgboost_test_pipeline,\n",
    "    pipeline_name=\"xgboost-train-pipeline\",\n",
    "    package_path=\"./test.json\",\n",
    "    type_check=True,\n",
    ")\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"testpipeline\",\n",
    "    template_path=\"./test.json\",\n",
    "    job_id=f'test-{id}',\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24 Jun Testing on TFDV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "import os\n",
    "\n",
    "\n",
    "stats1 = tfdv.load_statistics(\n",
    "    \"evaltest.pb\")\n",
    "stats2 = tfdv.load_statistics(\n",
    "\t\"evaltest.pb\"\n",
    ")\n",
    "schema1 = tfdv.infer_schema(statistics=stats1)\n",
    "schema2 = tfdv.infer_schema(statistics=stats2)\n",
    "\n",
    "tfdv.get_feature(schema1, 'default').drift_comparator.jensen_shannon_divergence.threshold = 0.01\n",
    "\n",
    "drift_anomalies = tfdv.validate_statistics(\n",
    "    statistics=stats2, schema=schema1, previous_statistics=stats1)\n",
    "print(drift_anomalies.drift_skew_info)\n",
    "\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "d = MessageToDict(drift_anomalies)\n",
    "\n",
    "val = d['driftSkewInfo'][0]['driftMeasurements'][0]['value']\n",
    "\n",
    "thresh = d['driftSkewInfo'][0]['driftMeasurements'][0]['threshold']\n",
    "print(val, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27cbf2f7b5893dafe4e8ecae5261b638a5ae255f0e4bf1f0d62d572ad7f511a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
