name: Bq extract data
inputs:
- {name: source_project_id, type: String}
- {name: source_table_url, type: String}
- {name: destination_project_id, type: String}
- {name: destination_bucket, type: String}
- {name: destination_file, type: String}
- {name: dataset_location, type: String}
- {name: extract_job_config, type: JsonObject, optional: true}
outputs:
- {name: dataset_gcs_uri, type: String}
- {name: dataset_gcs_directory, type: String}
implementation:
  container:
    image: gcr.io/pacific-torus-347809/mle-fp/base:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "from typing import NamedTuple\n\ndef bq_extract_data(\n    source_project_id,\n\
      \    source_table_url,\n    destination_project_id,\n    destination_bucket,\
      \ \n    destination_file,\n    dataset_location,\n    extract_job_config = None,\n\
      ):\n\n    import logging\n    import os\n\n    from google.cloud import bigquery,\
      \ storage\n    from google.cloud.exceptions import GoogleCloudError\n\n    os.environ[\"\
      GOOGLE_APPLICATION_CREDENTIALS\"] = \"pacific-torus.json\"\n\n    # logging.config.fileConfig(LOGGING_CONF)\n\
      \    logger = logging.getLogger(\"root\")\n\n    storage_client = storage.Client(project=destination_project_id)\n\
      \n    if not storage.Bucket(storage_client, destination_bucket).exists():\n\
      \        bucket = storage_client.bucket(destination_bucket)\n        bucket.storage_class\
      \ = \"STANDARD\" \n        storage_client.create_bucket(\n            bucket_or_name=bucket,\
      \ location=\"ASIA-SOUTHEAST1\", project=destination_project_id\n        )\n\
      \        logger.info(f\"Bucket created {destination_bucket}\")\n\n    full_table_url\
      \ = f\"{source_project_id}.{source_table_url}\"\n    table = bigquery.table.Table(table_ref=full_table_url)\n\
      \n    if extract_job_config is None:\n        extract_job_config = {}\n    if\
      \ destination_file.endswith(\".json\"):\n        extract_job_config = {\"destination_format\"\
      : \"NEWLINE_DELIMITED_JSON\"}\n    job_config = bigquery.ExtractJobConfig(**extract_job_config)\n\
      \n    dataset_gcs_uri = f\"gs://{destination_bucket}/{destination_file}\"\n\n\
      \    bq_client = bigquery.Client(project=destination_project_id)\n\n    logger.info(f\"\
      Extract {source_table_url} to {dataset_gcs_uri}\")\n    extract_job = bq_client.extract_table(\n\
      \        source=table,\n        destination_uris=dataset_gcs_uri,\n        job_config=job_config,\n\
      \        location=dataset_location,\n    )\n\n    dataset_gcs_directory = os.path.dirname(dataset_gcs_uri)\n\
      \n    try:\n        extract_job.result()\n        logger.info(f\"Table extracted:\
      \ {dataset_gcs_uri}\")\n    except GoogleCloudError as e:\n        logger.error(e)\n\
      \        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n\
      \        raise e\n\n    return (dataset_gcs_uri, dataset_gcs_directory)\n\n\
      def _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
      \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n\
      \            str(str_value), str(type(str_value))))\n    return str_value\n\n\
      import json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Bq extract\
      \ data', description='')\n_parser.add_argument(\"--source-project-id\", dest=\"\
      source_project_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --source-table-url\", dest=\"source_table_url\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--destination-project-id\", dest=\"destination_project_id\"\
      , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --destination-bucket\", dest=\"destination_bucket\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--destination-file\", dest=\"\
      destination_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --dataset-location\", dest=\"dataset_location\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--extract-job-config\", dest=\"extract_job_config\",\
      \ type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = bq_extract_data(**_parsed_args)\n\n_output_serializers =\
      \ [\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file\
      \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --source-project-id
    - {inputValue: source_project_id}
    - --source-table-url
    - {inputValue: source_table_url}
    - --destination-project-id
    - {inputValue: destination_project_id}
    - --destination-bucket
    - {inputValue: destination_bucket}
    - --destination-file
    - {inputValue: destination_file}
    - --dataset-location
    - {inputValue: dataset_location}
    - if:
        cond: {isPresent: extract_job_config}
        then:
        - --extract-job-config
        - {inputValue: extract_job_config}
    - '----output-paths'
    - {outputPath: dataset_gcs_uri}
    - {outputPath: dataset_gcs_directory}
