name: Model evaluation
inputs:
- {name: deployed_model_bucket, type: String}
- {name: trained_model, type: Model}
- {name: test_set, type: String}
- {name: threshold, type: Float}
- name: deploy
  type: String
  default: "False"
  optional: true
outputs:
- {name: deploy, type: String}
- {name: evaluated_model, type: Model}
implementation:
  container:
    image: gcr.io/pacific-torus-347809/mle-fp/base:latest
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'pandas' 'sklearn' 'xgboost' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
      install --quiet --no-warn-script-location 'pandas' 'sklearn' 'xgboost' --user)
      && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def model_evaluation(\n    deployed_model_bucket,\n    trained_model,\n   \
      \ test_set,\n    threshold,\n    deploy = 'False',\n):\n\n    from xgboost import\
      \ XGBClassifier\n    from google.cloud import storage\n    import pandas as\
      \ pd\n    from sklearn.metrics import roc_auc_score, confusion_matrix\n\n  \
      \  # Load Datasets and Models\n    data = pd.read_csv(test_set)\n    model =\
      \ XGBClassifier()\n    model.load_model(trained_model)\n\n    X = data.drop(['target'],\
      \ axis = 1)\n    Y = data['target']\n\n    y_prob = model.predict_proba(X)[:,\
      \ 1]\n    y_pred = model.predict(X)\n    auc = roc_auc_score(Y, y_prob)\n\n\
      \    trained_model.metadata['auc'] = auc\n    trained_model.metadata['confusion_matrix']\
      \ = {'classes': [0, 1], 'matrix': confusion_matrix(Y, y_pred).tolist()}\n\n\
      \    # Test versus Current Model and Threshold\n    client = storage.Client()\n\
      \    bucket = client.get_bucket(deployed_model_bucket)\n    blobs = list(client.list_blobs(bucket))\n\
      \n    if len(blobs) == 0 and auc > threshold:\n        deploy = 'True'\n   \
      \     return (deploy, trained_model)\n    elif len(blobs) == 1: \n        if\
      \ auc > blobs[0].metadata['auc']:\n            deploy = 'True'\n           \
      \ return (deploy, trained_model)\n\n    return (deploy, None)\n\ndef _serialize_str(str_value:\
      \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
      \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
      \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser =\
      \ argparse.ArgumentParser(prog='Model evaluation', description='')\n_parser.add_argument(\"\
      --deployed-model-bucket\", dest=\"deployed_model_bucket\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--trained-model\", dest=\"\
      trained_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --test-set\", dest=\"test_set\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--threshold\", dest=\"threshold\", type=float, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--deploy\", dest=\"deploy\"\
      , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = model_evaluation(**_parsed_args)\n\n_output_serializers\
      \ = [\n    _serialize_str,\n    str,\n\n]\n\nimport os\nfor idx, output_file\
      \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --deployed-model-bucket
    - {inputValue: deployed_model_bucket}
    - --trained-model
    - {inputPath: trained_model}
    - --test-set
    - {inputValue: test_set}
    - --threshold
    - {inputValue: threshold}
    - if:
        cond: {isPresent: deploy}
        then:
        - --deploy
        - {inputValue: deploy}
    - '----output-paths'
    - {outputPath: deploy}
    - {outputPath: evaluated_model}
